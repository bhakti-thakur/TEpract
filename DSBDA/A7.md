```
sample_text = """Natural Language Processing is a field of Artificial Intelligence 
that focuses on the interaction between humans and computers using natural language."""

import nltk
import string

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk import pos_tag
from nltk.stem import PorterStemmer, WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords') 
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('punkt_tab')
nltk.download('averaged_perceptron_tagger_eng')

tokens = word_tokenize(sample_text.lower())
print(tokens)
pos_tags = pos_tag(tokens)
print(pos_tags)
stop_words = set(stopwords.words('english'))
filtered_tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]
print("After stop_words' removal: ", filtered_tokens)

stemmer = PorterStemmer()
stemmed = [stemmer.stem(word) for word in filtered_tokens]
print("stemmed=", stemmed)

lemmatizer = WordNetLemmatizer()
lemmatized = [lemmatizer.lemmatize(word) for word in filtered_tokens]
print("Lemmatized:", lemmatized)

corpus = [
    "Natural Language Processing is a field of Artificial Intelligence.",
    "It focuses on the interaction between humans and computers.",
    "Using natural language, machines can understand human instructions."
]
vectorizer = TfidfVectorizer(stop_words= 'english')
x = vectorizer.fit_transform(corpus)
print('Tf idf matrix:')
print(x.toarray())
print("Vocabulary: ", vectorizer.get_feature_names_out())
```